{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPjHUnrcGfXlgsgbd/8okGm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mkNFi90lH64I","executionInfo":{"status":"ok","timestamp":1745872801716,"user_tz":420,"elapsed":9798,"user":{"displayName":"Erubiel Ramos","userId":"09437047253836357023"}},"outputId":"6eb23e8e-5d0c-4020-a7a4-0b0583b9946d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting imagehash\n","  Downloading ImageHash-4.3.2-py2.py3-none-any.whl.metadata (8.4 kB)\n","Collecting PyWavelets (from imagehash)\n","  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imagehash) (2.0.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from imagehash) (11.2.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from imagehash) (1.15.2)\n","Downloading ImageHash-4.3.2-py2.py3-none-any.whl (296 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyWavelets, imagehash\n","Successfully installed PyWavelets-1.8.0 imagehash-4.3.2\n"]}],"source":["!pip install imagehash"]},{"cell_type":"code","source":["import os\n","from pathlib import Path\n","import numpy as np\n","from PIL import Image\n","import imagehash\n","from collections import defaultdict\n","import pandas as pd\n","\n","def check_duplicate_images(paths):\n","    \"\"\"\n","    Check for duplicate images across train, valid, and test folders\n","    using both filename and image content comparison.\n","\n","    Args:\n","        paths (dict): Dictionary containing paths to train, valid, and test folders\n","\n","    Returns:\n","        tuple: (filename_duplicates, content_duplicates)\n","            - filename_duplicates: Dictionary of duplicate filenames\n","            - content_duplicates: Dictionary of duplicate images based on content\n","    \"\"\"\n","    # Store all filenames\n","    filename_map = defaultdict(list)\n","    # Store image hashes\n","    hash_map = defaultdict(list)\n","\n","    # Process each split (train/valid/test)\n","    for split in ['train', 'valid', 'test']:\n","        if os.path.exists(paths[split]):\n","            # Walk through all subdirectories\n","            for class_name in os.listdir(paths[split]):\n","                class_dir = os.path.join(paths[split], class_name)\n","                if os.path.isdir(class_dir):\n","                    # Process each image\n","                    for img_name in os.listdir(class_dir):\n","                        if img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n","                            # Store full path and split info for filename check\n","                            filename_map[img_name].append({\n","                                'path': os.path.join(class_dir, img_name),\n","                                'split': split,\n","                                'class': class_name\n","                            })\n","\n","                            # Calculate image hash for content comparison\n","                            try:\n","                                img_path = os.path.join(class_dir, img_name)\n","                                with Image.open(img_path) as img:\n","                                    # Convert to RGB if necessary\n","                                    if img.mode != 'RGB':\n","                                        img = img.convert('RGB')\n","                                    # Calculate perceptual hash\n","                                    img_hash = str(imagehash.average_hash(img))\n","                                    hash_map[img_hash].append({\n","                                        'path': img_path,\n","                                        'split': split,\n","                                        'class': class_name,\n","                                        'filename': img_name\n","                                    })\n","                            except Exception as e:\n","                                print(f\"Error processing {img_path}: {str(e)}\")\n","\n","    # Find duplicates by filename\n","    filename_duplicates = {\n","        filename: locations\n","        for filename, locations in filename_map.items()\n","        if len(locations) > 1\n","    }\n","\n","    # Find duplicates by content\n","    content_duplicates = {\n","        hash_val: locations\n","        for hash_val, locations in hash_map.items()\n","        if len(locations) > 1\n","    }\n","\n","    return filename_duplicates, content_duplicates\n","\n","def print_duplicate_summary(filename_duplicates, content_duplicates):\n","    \"\"\"Print a summary of found duplicates\"\"\"\n","    print(\"\\n=== Duplicate Analysis Summary ===\")\n","\n","    print(\"\\nDuplicates by filename:\")\n","    if filename_duplicates:\n","        for filename, locations in filename_duplicates.items():\n","            print(f\"\\nFilename: {filename}\")\n","            for loc in locations:\n","                print(f\"- Found in {loc['split']}/{loc['class']}\")\n","    else:\n","        print(\"No duplicate filenames found.\")\n","\n","    print(\"\\nDuplicates by content:\")\n","    if content_duplicates:\n","        for hash_val, locations in content_duplicates.items():\n","            print(f\"\\nHash: {hash_val}\")\n","            for loc in locations:\n","                print(f\"- {loc['filename']} in {loc['split']}/{loc['class']}\")\n","    else:\n","        print(\"No duplicate content found.\")\n","\n","def generate_duplicate_report(filename_duplicates, content_duplicates):\n","    \"\"\"Generate pandas DataFrames for detailed duplicate analysis\"\"\"\n","    # Prepare data for filename duplicates\n","    filename_data = []\n","    for filename, locations in filename_duplicates.items():\n","        for loc in locations:\n","            filename_data.append({\n","                'filename': filename,\n","                'split': loc['split'],\n","                'class': loc['class'],\n","                'full_path': loc['path']\n","            })\n","\n","    # Prepare data for content duplicates\n","    content_data = []\n","    for hash_val, locations in content_duplicates.items():\n","        for loc in locations:\n","            content_data.append({\n","                'hash': hash_val,\n","                'filename': loc['filename'],\n","                'split': loc['split'],\n","                'class': loc['class'],\n","                'full_path': loc['path']\n","            })\n","\n","    # Create DataFrames\n","    filename_df = pd.DataFrame(filename_data) if filename_data else pd.DataFrame()\n","    content_df = pd.DataFrame(content_data) if content_data else pd.DataFrame()\n","\n","    return filename_df, content_df\n","\n","def main():\n","    # Define base directory and paths\n","    base_dir = Path('/content/drive/MyDrive/CS471_AI/FinalProject/MangoDataset_Sorted_One')\n","    paths = {\n","        'base': base_dir,\n","        'valid': os.path.join(str(base_dir), 'valid'),\n","        'train': os.path.join(str(base_dir), 'train'),\n","        'test': os.path.join(str(base_dir), 'test')\n","    }\n","\n","    # Check for duplicates\n","    print(\"Checking for duplicate images...\")\n","    filename_duplicates, content_duplicates = check_duplicate_images(paths)\n","\n","    # Print summary\n","    print_duplicate_summary(filename_duplicates, content_duplicates)\n","\n","    # Generate detailed report\n","    filename_df, content_df = generate_duplicate_report(filename_duplicates, content_duplicates)\n","\n","    # Save reports if duplicates were found\n","    if not filename_df.empty:\n","        filename_df.to_csv('duplicate_filenames_report.csv', index=False)\n","        print(\"\\nDuplicate filenames report saved to 'duplicate_filenames_report.csv'\")\n","\n","    if not content_df.empty:\n","        content_df.to_csv('duplicate_content_report.csv', index=False)\n","        print(\"\\nDuplicate content report saved to 'duplicate_content_report.csv'\")\n","\n","    return filename_df, content_df\n","\n","if __name__ == \"__main__\":\n","    filename_df, content_df = main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8249caI_IAi3","executionInfo":{"status":"ok","timestamp":1745873644878,"user_tz":420,"elapsed":27,"user":{"displayName":"Erubiel Ramos","userId":"09437047253836357023"}},"outputId":"99615f3c-402c-4e81-a599-24b264c92e21"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking for duplicate images...\n","\n","=== Duplicate Analysis Summary ===\n","\n","Duplicates by filename:\n","No duplicate filenames found.\n","\n","Duplicates by content:\n","No duplicate content found.\n"]}]}]}